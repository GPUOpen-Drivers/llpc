; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --tool lgc --version 5
; RUN: lgc -o - -passes='require<lgc-pipeline-state>,function(lgc-lower-buffer-operations)' %s | FileCheck --check-prefixes=CHECK %s
;;
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
 ;
 ;  Copyright (c) 2024-2025 Advanced Micro Devices, Inc. All Rights Reserved.
 ;
 ;  Permission is hereby granted, free of charge, to any person obtaining a copy
 ;  of this software and associated documentation files (the "Software"), to
 ;  deal in the Software without restriction, including without limitation the
 ;  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 ;  sell copies of the Software, and to permit persons to whom the Software is
 ;  furnished to do so, subject to the following conditions:
 ;
 ;  The above copyright notice and this permission notice shall be included in all
 ;  copies or substantial portions of the Software.
 ;
 ;  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 ;  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 ;  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 ;  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 ;  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 ;  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 ;  IN THE SOFTWARE.
 ;
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

define amdgpu_gfx void @raw_atomic_load(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomic_load(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3:![0-9]+]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.amdgcn.raw.atomic.buffer.load.i32{{(\.v4i32)?}}(<4 x i32> [[DESC]], i32 0, i32 0, i32 5)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %load = load atomic i32, ptr addrspace(7) %ptr monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_xchg(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_xchg(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.swap.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %xchg = atomicrmw xchg ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_add(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_add(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.add.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %add = atomicrmw add ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_sub(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_sub(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.sub.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %sub = atomicrmw sub ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_and(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_and(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.and.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %and = atomicrmw and ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_or(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_or(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.or.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %or = atomicrmw or ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_xor(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_xor(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.xor.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %xor = atomicrmw xor ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_smax(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_smax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.smax.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %smax = atomicrmw max ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_smin(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_smin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.smin.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %smin = atomicrmw min ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_umax(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_umax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.umax.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %umax = atomicrmw umax ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_umin(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_umin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.amdgcn.raw.buffer.atomic.umin.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %umin = atomicrmw umin ptr addrspace(7) %ptr, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_fadd(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_fadd(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.amdgcn.raw.buffer.atomic.fadd.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %fadd = atomicrmw fadd ptr addrspace(7) %ptr, float 1.0 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_fmax(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_fmax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.amdgcn.raw.buffer.atomic.fmax.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %fmax = atomicrmw fmax ptr addrspace(7) %ptr, float 1.0 monotonic, align 8
  ret void
}

define amdgpu_gfx void @raw_atomicrmw_fmin(<4 x i32> inreg %desc) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @raw_atomicrmw_fmin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.amdgcn.raw.buffer.atomic.fmin.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[DESC]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %fmin = atomicrmw fmin ptr addrspace(7) %ptr, float 1.0 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomic_load(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomic_load(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i32 @llvm.amdgcn.struct.atomic.buffer.load.i32{{(\.v4i32)?}}(<4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 5)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %load = load atomic i32, ptr addrspace(9) %struct.ptr.idx monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_xchg(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_xchg(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.swap.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %xchg = atomicrmw xchg ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_add(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_add(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.add.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %add = atomicrmw add ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_sub(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_sub(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.sub.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %sub = atomicrmw sub ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_and(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_and(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.and.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %and = atomicrmw and ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_or(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_or(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.or.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %or = atomicrmw or ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_xor(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_xor(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.xor.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %xor = atomicrmw xor ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_smax(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_smax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.smax.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %smax = atomicrmw max ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_smin(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_smin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.smin.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %smin = atomicrmw min ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_umax(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_umax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.umax.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %umax = atomicrmw umax ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_umin(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_umin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call i64 @llvm.amdgcn.struct.buffer.atomic.umin.i64{{(\.v4i32)?}}(i64 1, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %umin = atomicrmw umin ptr addrspace(9) %struct.ptr.idx, i64 1 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_fadd(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_fadd(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call float @llvm.amdgcn.struct.buffer.atomic.fadd.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %fadd = atomicrmw fadd ptr addrspace(9) %struct.ptr.idx, float 1.0 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_fmax(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_fmax(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call float @llvm.amdgcn.struct.buffer.atomic.fmax.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %fmax = atomicrmw fmax ptr addrspace(9) %struct.ptr.idx, float 1.0 monotonic, align 8
  ret void
}

define amdgpu_gfx void @struct_atomicrmw_fmin(<4 x i32> inreg %desc, i32 %index) !lgc.shaderstage !0 {
; CHECK-LABEL: define amdgpu_gfx void @struct_atomicrmw_fmin(
; CHECK-SAME: <4 x i32> inreg [[DESC:%.*]], i32 [[INDEX:%.*]]) !lgc.shaderstage [[META3]] {
; CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[DESC]], i64 1
; CHECK-NEXT:    [[TMP2:%.*]] = and i32 [[TMP1]], -1073676289
; CHECK-NEXT:    [[TMP3:%.*]] = or i32 [[TMP2]], 524288
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <4 x i32> [[DESC]], i32 [[TMP3]], i64 1
; CHECK-NEXT:    [[TMP8:%.*]] = extractelement <4 x i32> [[TMP4]], i64 3
; CHECK-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], -805306369
; CHECK-NEXT:    [[TMP7:%.*]] = or i32 [[TMP9]], 536870912
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i32> [[TMP4]], i32 [[TMP7]], i64 3
; CHECK-NEXT:    [[TMP11:%.*]] = call float @llvm.amdgcn.struct.buffer.atomic.fmin.f32{{(\.v4i32)?}}(float 1.000000e+00, <4 x i32> [[TMP10]], i32 [[INDEX]], i32 0, i32 0, i32 0)
; CHECK-NEXT:    ret void
;
  %ptr = call ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32> %desc, i1 false)
  %struct.ptr = call ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7) %ptr, i32 8)
  %struct.ptr.idx = call ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9) %struct.ptr, i32 %index)
  %fmin = atomicrmw fmin ptr addrspace(9) %struct.ptr.idx, float 1.0 monotonic, align 8
  ret void
}

declare ptr addrspace(7) @lgc.buffer.desc.to.ptr(<4 x i32>, i1) nounwind readnone
declare ptr addrspace(7) @lgc.buffer.index(ptr addrspace(7), i32, i32)
declare ptr addrspace(9) @lgc.convert.to.strided.buffer.pointer(ptr addrspace(7), i32)
declare ptr addrspace(9) @lgc.strided.index.add(ptr addrspace(9), i32)

!0 = !{i32 7}
;.
; CHECK: [[META3]] = !{i32 7}
;.
